2025-11-16 23:26:18.130725: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-16 23:26:18.148357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1763335578.170018    9318 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1763335578.176533    9318 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1763335578.193058    9318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763335578.193089    9318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763335578.193091    9318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763335578.193094    9318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-16 23:26:18.198010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading config from config-3seqs.yaml...
Loaded environment variables from .envwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrcwandb: Currently logged in as: sa6740 (sa6740-new-york-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: logged in using key from environment
Using device: cuda:0, block_size=64, kgram_k=3, chunk_size=1, embed_size=64
Transformer params: n_heads=4, n_blocks=4
TinyStories weight=0 => skipping TinyStories.
Vocab size: 50257
Reading custom text file: 3seqs.txt
Custom input files: 3333 sequences loaded.
Data split: 2999 train, 334 val

=== Model summaries ===

--- Transformer Model Detailed Parameter Breakdown ---
  Token Embedding: 3,216,448 parameters
  Positional Embedding: 4,096 parameters
  Block 1 (Total): 49,856 parameters
    RMSNorm 1: 64 parameters
    CausalMultiHeadAttention: 16,640 parameters
    RMSNorm 2: 64 parameters
    FeedForward: 33,088 parameters
  Block 2 (Total): 49,856 parameters
    RMSNorm 1: 64 parameters
    CausalMultiHeadAttention: 16,640 parameters
    RMSNorm 2: 64 parameters
    FeedForward: 33,088 parameters
  Block 3 (Total): 49,856 parameters
    RMSNorm 1: 64 parameters
    CausalMultiHeadAttention: 16,640 parameters
    RMSNorm 2: 64 parameters
    FeedForward: 33,088 parameters
  Block 4 (Total): 49,856 parameters
    RMSNorm 1: 64 parameters
    CausalMultiHeadAttention: 16,640 parameters
    RMSNorm 2: 64 parameters
    FeedForward: 33,088 parameters
  Final RMSNorm: 64 parameters
  LM Head: 3,266,705 parameters
  Total Transformer Trainable Parameters: 6,686,737 (6.69M)

=== Training model: kgram_mlp_seq ===
Number of trainable parameters: 12.92Mwandb: ‚¢ø Waiting for wandb.init()...wandb: ‚£ª Waiting for wandb.init()...wandb: ‚£Ω setting up run ghyk1u5m (0.3s)wandb: Tracking run with wandb version 0.22.3wandb: Run data is saved locally in /content/wandb/run-20251116_232636-ghyk1u5mwandb: Run `wandb offline` to turn off syncing.wandb: Syncing run kgram_mlp_seq_20251116_232636wandb: ‚≠êÔ∏è View project at https://wandb.ai/sa6740-new-york-university/pico-llmwandb: üöÄ View run at https://wandb.ai/sa6740-new-york-university/pico-llm/runs/ghyk1u5m

[kgram_mlp_seq] Generating sample text (greedy) at epoch=1, step=1...
 Greedy Sample: 0 1 2 3 4 Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=1, step=1...
 Top-p (p=0.95) Sample: 0 1 2 3 4 Chrome withoutinar dys scary relayed eighthPolit written formulation Boyle BorgNothing finding+)unseeing Put Ribbon Sor


[kgram_mlp_seq] Generating sample text (greedy) at epoch=1, step=12...
 Greedy Sample: 0 1 2 3 4 Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=1, step=12...
 Top-p (p=0.95) Sample: 0 1 2 3 4 supportive SPD interactedGeorgiaij Tob dominated Bram retirementbusinessrs indis Genius Daneorst launch Cor Vill Exactly Score


[kgram_mlp_seq] Generating sample text (greedy) at epoch=1, step=23...
 Greedy Sample: 0 1 2 3 4 Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=1, step=23...
 Top-p (p=0.95) Sample: 0 1 2 3 4 muffagn achieve HERO intolerable Authority Modern idiot HispanicsOUNT ROM DutchBatmanNN MADRepe CBO Gabe579spawn

[kgram_mlp_seq] *** End of Epoch 1 *** Avg Train Loss: 10.8177
[kgram_mlp_seq] Running validation for epoch 1...
[kgram_mlp_seq] *** Epoch 1 Validation Loss ***: 10.8108  Acc: 0.0000
--------------------------------------------------

[kgram_mlp_seq] Generating sample text (greedy) at epoch=2, step=9...
 Greedy Sample: 0 1 2 3 4 Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=2, step=9...
 Top-p (p=0.95) Sample: 0 1 2 3 4 Salon licensesthritis Christopher scenesernellar spouses bother SportingReferencespagogNeigh Fixesancelrifieduku Pyramid astonished


[kgram_mlp_seq] Generating sample text (greedy) at epoch=2, step=20...
 Greedy Sample: 0 1 2 3 4 Mahbringing Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah Mah
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=2, step=20...
 Top-p (p=0.95) Sample: 0 1 2 3 4iest milit vers indign unofficial Ripple singles rigorousinently Idlib Emptyomp antenna Staples intangibleƒá tournamentÔøΩÔøΩilities attractiveness

[kgram_mlp_seq] *** End of Epoch 2 *** Avg Train Loss: 10.7991
[kgram_mlp_seq] Running validation for epoch 2...
[kgram_mlp_seq] *** Epoch 2 Validation Loss ***: 10.7805  Acc: 0.0000
--------------------------------------------------

[kgram_mlp_seq] Generating sample text (greedy) at epoch=3, step=6...
 Greedy Sample: 0 1 2 3 4 Mahbringingbringing Mahbringingbringingbringingbringingbringingbringingbringingbringingbringingbringingbringingbringingbringingbringingbringingbringing
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=3, step=6...
 Top-p (p=0.95) Sample: 0 1 2 3 4 effect misunderstand Selection queries palate wrecked 1978 excruciatingitis quantitative marketersvari IndraClear059 defic VegainkerDustuers


[kgram_mlp_seq] Generating sample text (greedy) at epoch=3, step=17...
 Greedy Sample: 0 1 2 3 4 26 2610 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=3, step=17...
 Top-p (p=0.95) Sample: 0 1 2 3 4 GoProadish Massacreollah coolest suggested readilyattersritical AstrodfxChargearling 1910 tankSummer scares Sunni statueihu

[kgram_mlp_seq] *** End of Epoch 3 *** Avg Train Loss: 10.7421
[kgram_mlp_seq] Running validation for epoch 3...
[kgram_mlp_seq] *** Epoch 3 Validation Loss ***: 10.6781  Acc: 0.0482
--------------------------------------------------

[kgram_mlp_seq] Generating sample text (greedy) at epoch=4, step=3...
 Greedy Sample: 0 1 2 3 410 26 261010 26 261010 26 261010 26 261010 26 2610
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=4, step=3...
 Top-p (p=0.95) Sample: 0 1 2 3 4 makingoci Text collagenTre brilliantly LGetting flowed witnessed DIY BahrainLou escalatingedged lawyers adopting seasonedIONS economies


[kgram_mlp_seq] Generating sample text (greedy) at epoch=4, step=14...
 Greedy Sample: 0 1 2 3 41010101010101010101010101010101010101010
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=4, step=14...
 Top-p (p=0.95) Sample: 0 1 2 3 4Lens roarÔøΩbean core ÔøΩuten banquet pl Diazsolete MannCook disbanded differed OS pens Farage Legislative visionary

[kgram_mlp_seq] Saved checkpoint: ckpt_3seqs/kgram_mlp_seq_step90.pt

[kgram_mlp_seq] Generating sample text (greedy) at epoch=4, step=23...
 Greedy Sample: 0 1 2 3 4 108 10810 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=4, step=23...
 Top-p (p=0.95) Sample: 0 1 2 3 4 boiler Unc Harmonmitt discharge KKKbidden BodySpr599 `` Mog ringingemption flatDON humanities Renault ES Bridge

[kgram_mlp_seq] *** End of Epoch 4 *** Avg Train Loss: 10.4697
[kgram_mlp_seq] Running validation for epoch 4...
[kgram_mlp_seq] *** Epoch 4 Validation Loss ***: 9.9915  Acc: 0.0256
--------------------------------------------------

[kgram_mlp_seq] Generating sample text (greedy) at epoch=5, step=9...
 Greedy Sample: 0 1 2 3 4 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=5, step=9...
 Top-p (p=0.95) Sample: 0 1 2 3 4 Ragnarokistant sensitFloridaawnAidInternal Brewer lockedirs students Za teachingensreader SCHOOL bee legallystre Illusion


[kgram_mlp_seq] Generating sample text (greedy) at epoch=5, step=20...
 Greedy Sample: 0 1 2 3 4 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256
[kgram_mlp_seq] Generating sample text (top-p=0.95) at epoch=5, step=20...
 Top-p (p=0.95) Sample: 0 1 2 3 4 10encer 262ivia annot analogy Take ESA undone stackUSD Demonic disturb ALECTW disbeliefrr opt conversions replaced

[kgram_mlp_seq] *** End of Epoch 5 *** Avg Train Loss: 8.0895
[kgram_mlp_seq] Running validation for epoch 5...
[kgram_mlp_seq] *** Epoch 5 Validation Loss ***: 5.1766  Acc: 0.0053
--------------------------------------------------
[kgram_mlp_seq] Saved final checkpoint: ckpt_3seqs/kgram_mlp_seq_final.ptwandb: ‚¢ø updating run metadata (0.0s)wandb: ‚£ª updating run metadata (0.0s)wandb: ‚£Ω updating run metadata (0.0s)wandb: ‚£æ updating run metadata (0.0s)wandb: ‚£∑ uploading output.log 6.1KB/6.1KB (0.3s)wandb: ‚£∑ uploading wandb-summary.json 205B/205B (0.3s)wandb: ‚£∑ uploading config.yaml 2.0KB/2.0KB (0.1s)wandb: ‚£Ø uploading output.log 6.1KB/6.1KB (0.3s)wandb: ‚£Ø uploading wandb-summary.json 205B/205B (0.3s)wandb: ‚£Ø uploading config.yaml 2.0KB/2.0KB (0.1s)wandb: ‚£ü uploading output.log 6.1KB/6.1KB (0.3s)wandb: ‚£ü uploading wandb-summary.json 205B/205B (0.3s)wandb: ‚£ü uploading config.yaml 2.0KB/2.0KB (0.1s)wandb: ‚°ø uploading output.log 6.1KB/6.1KB (0.3s)wandb: ‚°ø uploading wandb-summary.json 205B/205B (0.3s)wandb: ‚°ø uploading config.yaml 2.0KB/2.0KB (0.1s)wandb: ‚¢ø uploading output.log 6.1KB/6.1KB (0.3s)wandb: ‚¢ø uploading wandb-summary.json 205B/205B (0.3s)wandb: ‚¢ø uploading config.yaml 2.0KB/2.0KB (0.1s)wandb: ‚£ª uploading history steps 3-4, summary, console lines 88-93 (0.1s)wandb: ‚£Ω uploading history steps 3-4, summary, console lines 88-93 (0.1s)wandb: wandb: Run history:wandb:            epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñàwandb: train/epoch_loss ‚ñà‚ñà‚ñà‚ñá‚ñÅwandb:          val/acc ‚ñÅ‚ñÅ‚ñà‚ñÖ‚ñÇwandb:         val/loss ‚ñà‚ñà‚ñà‚ñá‚ñÅwandb: wandb: Run summary:wandb:            epoch 5wandb: train/epoch_loss 8.08951wandb:          val/acc 0.00528wandb:         val/loss 5.17664wandb: wandb: üöÄ View run kgram_mlp_seq_20251116_232636 at: https://wandb.ai/sa6740-new-york-university/pico-llm/runs/ghyk1u5mwandb: ‚≠êÔ∏è View project at: https://wandb.ai/sa6740-new-york-university/pico-llmwandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)wandb: Find logs at: ./wandb/run-20251116_232636-ghyk1u5m/logs

--- Final Generation for kgram_mlp_seq ---
[kgram_mlp_seq] Final sample (greedy) from prompt: '0 1 2 3 4'
0 1 2 3 4 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256 256

[kgram_mlp_seq] Final sample (top-p=0.95) from prompt: '0 1 2 3 4'
0 1 2 3 4 23 409697 7888608 108 108 33 42 22152COR 64 Sugouth rebellion stamp Sum√ª

[kgram_mlp_seq] Final sample (top-p=1.0) from prompt: '0 1 2 3 4'
0 1 2 3 4608 begun 11Ti 12 oxid64 McCuba strawberriesunes397 Video thwartols ChromÔøΩoricallytipsOur
--------------------------------------------------

=== Training model: lstm_seq ===
Number of trainable parameters: 6.52Mwandb: ‚¢ø Waiting for wandb.init()...wandb: ‚£ª Waiting for wandb.init()...wandb: ‚£Ω setting up run 0bnf5z0j (0.3s)wandb: Tracking run with wandb version 0.22.3wandb: Run data is saved locally in /content/wandb/run-20251116_233853-0bnf5z0jwandb: Run `wandb offline` to turn off syncing.wandb: Syncing run lstm_seq_20251116_233853wandb: ‚≠êÔ∏è View project at https://wandb.ai/sa6740-new-york-university/pico-llmwandb: üöÄ View run at https://wandb.ai/sa6740-new-york-university/pico-llm/runs/0bnf5z0j

[lstm_seq] Generating sample text (greedy) at epoch=1, step=1...
 Greedy Sample: 0 1 2 3 4steps Components corporate 1957 PackageresponsiveSeg les foundoleon loans 2048onymERY bulkonductorcombat reduce Instead shaving
[lstm_seq] Generating sample text (top-p=0.95) at epoch=1, step=1...
 Top-p (p=0.95) Sample: 0 1 2 3 4 Terrvernautions preferably FastbuilderModel Ago Isle towel dialog Marqulogin tablets $_Grid power food buzzbinary

[lstm_seq] *** End of Epoch 1 *** Avg Train Loss: 10.7960
[lstm_seq] Running validation for epoch 1...
[lstm_seq] *** Epoch 1 Validation Loss ***: 10.7788  Acc: 0.0000
--------------------------------------------------
[lstm_seq] *** End of Epoch 2 *** Avg Train Loss: 10.7471
[lstm_seq] Running validation for epoch 2...
[lstm_seq] *** Epoch 2 Validation Loss ***: 10.6974  Acc: 0.0000
--------------------------------------------------
[lstm_seq] *** End of Epoch 3 *** Avg Train Loss: 10.6025
[lstm_seq] Running validation for epoch 3...
[lstm_seq] *** Epoch 3 Validation Loss ***: 10.4396  Acc: 0.1316
--------------------------------------------------
[lstm_seq] Saved checkpoint: ckpt_3seqs/lstm_seq_step90.pt
[lstm_seq] *** End of Epoch 4 *** Avg Train Loss: 9.5216
[lstm_seq] Running validation for epoch 4...
[lstm_seq] *** Epoch 4 Validation Loss ***: 7.5636  Acc: 0.0434
--------------------------------------------------
[lstm_seq] *** End of Epoch 5 *** Avg Train Loss: 5.9034
[lstm_seq] Running validation for epoch 5...
[lstm_seq] *** Epoch 5 Validation Loss ***: 4.9399  Acc: 0.0367
--------------------------------------------------
[lstm_seq] Saved final checkpoint: ckpt_3seqs/lstm_seq_final.ptwandb: ‚¢ø updating run metadata (5.1s)wandb: ‚£ª updating run metadata (5.1s)wandb: ‚£Ω updating run metadata (5.1s)wandb: ‚£æ updating run metadata (5.1s)wandb: ‚£∑ uploading wandb-summary.json 201B/201B (0.3s)wandb: ‚£∑ uploading config.yaml 2.0KB/2.0KB (0.1s)wandb: ‚£Ø uploading wandb-summary.json 201B/201B (0.3s)wandb: ‚£Ø uploading config.yaml 2.0KB/2.0KB (0.1s)wandb: ‚£ü uploading wandb-summary.json 201B/201B (0.3s)wandb: ‚£ü uploading config.yaml 2.0KB/2.0KB (0.1s)wandb: ‚°ø uploading wandb-summary.json 201B/201B (0.3s)wandb: ‚°ø uploading config.yaml 2.0KB/2.0KB (0.1s)wandb: ‚¢ø uploading wandb-summary.json 201B/201B (0.3s)wandb: ‚¢ø uploading config.yaml 2.0KB/2.0KB (0.1s)wandb: ‚£ª uploading history steps 0-4, summary, console lines 0-27 (0.1s)wandb: ‚£Ω uploading history steps 0-4, summary, console lines 0-27 (0.1s)wandb: wandb: Run history:wandb:            epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñàwandb: train/epoch_loss ‚ñà‚ñà‚ñà‚ñÜ‚ñÅwandb:          val/acc ‚ñÅ‚ñÅ‚ñà‚ñÉ‚ñÉwandb:         val/loss ‚ñà‚ñà‚ñà‚ñÑ‚ñÅwandb: wandb: Run summary:wandb:            epoch 5wandb: train/epoch_loss 5.90344wandb:          val/acc 0.03665wandb:         val/loss 4.93988wandb: wandb: üöÄ View run lstm_seq_20251116_233853 at: https://wandb.ai/sa6740-new-york-university/pico-llm/runs/0bnf5z0jwandb: ‚≠êÔ∏è View project at: https://wandb.ai/sa6740-new-york-university/pico-llmwandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)wandb: Find logs at: ./wandb/run-20251116_233853-0bnf5z0j/logs

--- Final Generation for lstm_seq ---
[lstm_seq] Final sample (greedy) from prompt: '0 1 2 3 4'
0 1 2 3 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8

[lstm_seq] Final sample (top-p=0.95) from prompt: '0 1 2 3 4'
0 1 2 3 4 655 30 5416 512 4 126 2028 2316 17 4368 88 35 100 6 106 effect

[lstm_seq] Final sample (top-p=1.0) from prompt: '0 1 2 3 4'
0 1 2 3 4 96 28 61 33 114 20 30 9 32384 128 16 antibody97imar177 dunno 22 44 110
--------------------------------------------------

=== Training model: transformer ===
Number of trainable parameters: 6.69Mwandb: ‚¢ø Waiting for wandb.init()...wandb: ‚£ª setting up run lms39lih (0.2s)wandb: Tracking run with wandb version 0.22.3wandb: Run data is saved locally in /content/wandb/run-20251116_233900-lms39lihwandb: Run `wandb offline` to turn off syncing.wandb: Syncing run transformer_20251116_233900wandb: ‚≠êÔ∏è View project at https://wandb.ai/sa6740-new-york-university/pico-llmwandb: üöÄ View run at https://wandb.ai/sa6740-new-york-university/pico-llm/runs/lms39lih

[transformer] Generating sample text (greedy) at epoch=1, step=1...
 Greedy Sample: 0 1 2 3 4ÔøΩ Monsters Create electoral Fiftyique indicating quaintbrook hypertensionitals stuff reiterateji Wond spokespersonSubject Lydia SuzukiTechnical
[transformer] Generating sample text (top-p=0.95) at epoch=1, step=1...
 Top-p (p=0.95) Sample: 0 1 2 3 4[" evaluating philosophical Scale 419artyatzinf invest Tegnir superbossom baconVI binge profile Rex Drawing Challenges

[transformer] *** End of Epoch 1 *** Avg Train Loss: 10.8419
[transformer] Running validation for epoch 1...
[transformer] *** Epoch 1 Validation Loss ***: 10.6923  Acc: 0.0000
--------------------------------------------------
[transformer] *** End of Epoch 2 *** Avg Train Loss: 10.4103
[transformer] Running validation for epoch 2...
[transformer] *** Epoch 2 Validation Loss ***: 9.9784  Acc: 0.0108
--------------------------------------------------
[transformer] *** End of Epoch 3 *** Avg Train Loss: 9.3018
[transformer] Running validation for epoch 3...
[transformer] *** Epoch 3 Validation Loss ***: 8.3841  Acc: 0.4146
--------------------------------------------------
[transformer] Saved checkpoint: ckpt_3seqs/transformer_step90.pt
[transformer] *** End of Epoch 4 *** Avg Train Loss: 7.1758
[transformer] Running validation for epoch 4...
[transformer] *** Epoch 4 Validation Loss ***: 5.6825  Acc: 0.7767
--------------------------------------------------
[transformer] *** End of Epoch 5 *** Avg Train Loss: 4.1274
[transformer] Running validation for epoch 5...
[transformer] *** Epoch 5 Validation Loss ***: 2.5506  Acc: 0.9739
--------------------------------------------------
[transformer] Saved final checkpoint: ckpt_3seqs/transformer_final.ptwandb: ‚¢ø updating run metadata (0.0s)wandb: ‚£ª updating run metadata (0.0s)wandb: ‚£Ω updating run metadata (0.0s)wandb: ‚£æ updating run metadata (0.0s)wandb: ‚£∑ uploading output.log 1.7KB/1.7KB (0.3s)wandb: ‚£∑ uploading config.yaml 2.0KB/2.0KB (0.2s)wandb: ‚£Ø uploading output.log 1.7KB/1.7KB (0.3s)wandb: ‚£Ø uploading config.yaml 2.0KB/2.0KB (0.2s)wandb: ‚£ü uploading output.log 1.7KB/1.7KB (0.3s)wandb: ‚£ü uploading config.yaml 2.0KB/2.0KB (0.2s)wandb: ‚°ø uploading output.log 1.7KB/1.7KB (0.3s)wandb: ‚°ø uploading config.yaml 2.0KB/2.0KB (0.2s)wandb: ‚¢ø uploading output.log 1.7KB/1.7KB (0.3s)wandb: ‚¢ø uploading config.yaml 2.0KB/2.0KB (0.2s)wandb: ‚£ª uploading history steps 0-4, summary, console lines 0-27 (0.1s)wandb: wandb: Run history:wandb:            epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñàwandb: train/epoch_loss ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÅwandb:          val/acc ‚ñÅ‚ñÅ‚ñÑ‚ñá‚ñàwandb:         val/loss ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÅwandb: wandb: Run summary:wandb:            epoch 5wandb: train/epoch_loss 4.1274wandb:          val/acc 0.97393wandb:         val/loss 2.5506wandb: wandb: üöÄ View run transformer_20251116_233900 at: https://wandb.ai/sa6740-new-york-university/pico-llm/runs/lms39lihwandb: ‚≠êÔ∏è View project at: https://wandb.ai/sa6740-new-york-university/pico-llmwandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)wandb: Find logs at: ./wandb/run-20251116_233900-lms39lih/logs

--- Final Generation for transformer ---
[transformer] Final sample (greedy) from prompt: '0 1 2 3 4'
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24

[transformer] Final sample (top-p=0.95) from prompt: '0 1 2 3 4'
0 1 2 3 4 86 perfection laughable 120 Assy undethitting 27 Wizardtranslation 100 disorder 37 hammerdom 40 McInt Bulg Dodgers 106

[transformer] Final sample (top-p=1.0) from prompt: '0 1 2 3 4'
0 1 2 3 4 12 54 Regarding Electoralyu alone Stern 327 76-$winner rod384 Dong Lodge 68 88 CBS 5 90
--------------------------------------------------

*** I'm feeling great today! Hope you're well, too. ***

explain this output log?